{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Discovering_recommenders.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh46pLURUxL8"
      },
      "source": [
        "# Welcome to my collaborative filtering recommender Notebook\n",
        "The aim of this notebook is to implement different collaborative filtering algorithms in order to recommand new items (movies in our case). \n",
        "\n",
        "Collaborative filtering aims to select the items that a user might like based on\n",
        "the reactions, choices and tastes of similar users.\n",
        "\n",
        "Collaborative filtering can be divided into two main steps:\n",
        "*   Find similar users to a user 'A'\n",
        "*   Predict the ratings of the items that are not yet rated by 'A\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nolK1iRJdEV0"
      },
      "source": [
        "## Packages used in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SS3_-0CMyB5",
        "outputId": "6de2dee8-6a11-437d-bde2-ae20192fa638"
      },
      "source": [
        "!pip install numpy\n",
        "!pip install scikit-surprise"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 49 kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1619429 sha256=4c582a87434e8c939816762813c908cfd32964510350fd6876cb1fe3cdab67c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL9_Ih8uVLZj"
      },
      "source": [
        "## Importing the 'MovieLens 100k' Dataset\n",
        "\n",
        "The dataset contains 100,000 ratings ranging from 1 to 5, from 943 users on 1682 movies. <br>\n",
        "Each user has rated at least 20 movies.<br>\n",
        "It was released in April 1998."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjmMAxfHNBOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b312ee60-9764-461c-f58a-bfe6e1afab00"
      },
      "source": [
        "from surprise.model_selection import train_test_split\n",
        "from surprise import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Loads the builtin Movielens-100k data\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# 'test_set' is made of 20% of the ratings.\n",
        "train_set, test_set = train_test_split(data, test_size=.2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset ml-100k could not be found. Do you want to download it? [Y/n] Y\n",
            "Trying to download dataset from http://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
            "Done! Dataset ml-100k has been saved to /root/.surprise_data/ml-100k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucS335NLVA4n"
      },
      "source": [
        "## Memory-based Approach\n",
        "\n",
        "The main idea behind these methods is to N find similar users or items using the concept of similarity. We can use different tools like 'cosine similarity' or 'euclidien distance'.<br>\n",
        "\n",
        "I used the <font color='blue'>'K Nearest Neighbours'</font> algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd2D0Vz-NPcs"
      },
      "source": [
        "from surprise import KNNWithMeans\n",
        "\n",
        "# To use item-based cosine similarity\n",
        "sim_options = {\n",
        "    \"name\": \"cosine\",\n",
        "    \"user_based\": False,  # Compute  similarities between items\n",
        "    \"min_support\": 5, # The minimum number of common items/users to consider them for similarity\n",
        "}\n",
        "\n",
        "# Memory_based_model\n",
        "knn = KNNWithMeans(sim_options=sim_options)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX7sLgQEVSJ1"
      },
      "source": [
        "Training and evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuvAJQkySFuy",
        "outputId": "d54f8975-9fa3-44fe-eaa2-a28642bf3b8a"
      },
      "source": [
        "from surprise import accuracy\n",
        "\n",
        "knn.fit(train_set)\n",
        "\n",
        "# 'test' method estimates all the ratings in the given test_set\n",
        "knn_pred = knn.test(test_set)\n",
        "\n",
        "# Evaluation Metric 'Accuracy'\n",
        "accuracy.rmse(knn_pred)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "RMSE: 0.9410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9410064754499461"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB3BOQztVZz3"
      },
      "source": [
        "# Matrix Factorization Model\n",
        "Like any other Matrix Factorization algorithm, the idea here is to break down a large sparse matrix into a product of smaller ones.\n",
        "\n",
        "for example, when breaking (M,N) matrix into two matrices (M,K) and (K,N) : \n",
        "K represents the number of latent factors, every latent factor is an indication of hidden characteristics about the users or the items.\n",
        "\n",
        "I will use <font color='blue'>'Single Value Decomposition Matrix Factorization'</font>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rbm-d6hPGEX"
      },
      "source": [
        "from surprise import SVD\n",
        "\n",
        "#Matrix factorization model\n",
        "svd = SVD(n_epochs=10, lr_all=.005)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ge6xmicUbFL",
        "outputId": "8128570e-92b9-4178-e295-f3b2589cb7dc"
      },
      "source": [
        "from surprise import accuracy\n",
        "\n",
        "svd.fit(train_set)\n",
        "svd_pred = svd.test(test_set)\n",
        "\n",
        "# Evaluation Metric 'Accuracy'\n",
        "accuracy.rmse(svd_pred)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.9445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9445436383704072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aevRlUBVhK6"
      },
      "source": [
        "# Fine Tuning the params of the model using Grid Search method\n",
        "GridSearchCv uses grid search method with cross validation, which simply consist on trying all the possible given combinations (2 x 3 x 2 = 12 in our case) with cross validation.\n",
        "\n",
        "Since cross validation is integrated in the implementation, we dont need to split data to train and test (because the function will do it in the CV part), we will pass the whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac5ZbPNUV195",
        "outputId": "87472cae-aa26-469f-d664-e68e2357bc27"
      },
      "source": [
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "# We use the whole dataset\n",
        "all_dataset = data\n",
        "\n",
        "param_grid = {\n",
        "    \"n_epochs\": [5, 10],\n",
        "    \"lr_all\": [0.002, 0.004, 0.006],\n",
        "    \"reg_all\": [0.2, 0.4, 0.6]\n",
        "}\n",
        "\n",
        "# We set the CV to 4, the dataset will be splitted into 4 chunks, and will be tested on each one of them (after training on the others)\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=4) \n",
        "gs.fit(all_dataset)\n",
        "\n",
        "print(gs.best_score[\"rmse\"])\n",
        "print(gs.best_params[\"rmse\"])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9513160132449412\n",
            "{'n_epochs': 10, 'lr_all': 0.006, 'reg_all': 0.2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCmNNtgahYWZ"
      },
      "source": [
        "## Further improvements\n",
        "\n",
        "We can use different metrics in order to evaluate our models.<br>\n",
        "\n",
        "* Using 'recall' will enable us to minimize the number of \"good movies that were not chosen\".\n",
        "* Using 'precision' will enable us to reduce the number of \"bad movies that were recommended\"."
      ]
    }
  ]
}